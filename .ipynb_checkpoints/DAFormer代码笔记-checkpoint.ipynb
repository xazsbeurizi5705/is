{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c6d420",
   "metadata": {},
   "source": [
    "# DAFormer数据处理\n",
    "\n",
    "语义分割数据的处理比较简单，需要把img处理成带标签的数据，然后主要是稀有类抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8517c7",
   "metadata": {},
   "source": [
    "## Cityscapes数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a562709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 19:06:09,039 - mmseg - INFO - Loaded 2975 images from /raid/wzq/data/cityscapes/leftImg8bit/train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.img_metas: /raid/wzq/data/cityscapes/leftImg8bit/train/tubingen/tubingen_000110_000019_leftImg8bit.png\n",
      "gt tensor([[[[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]]],\n",
      "\n",
      "\n",
      "        [[[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]]]])\n",
      "gt.size torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from mmseg.datasets.cityscapes import CityscapesDataset\n",
    "from mmseg.datasets.builder import build_dataset\n",
    "from mmseg.datasets.builder import build_dataloader\n",
    "from mmcv.utils.config import Config\n",
    "\n",
    "cfg_file = './configs/_base_/datasets/cityscapes_half_512x512.py'\n",
    "data_cfg = Config.fromfile(cfg_file)\n",
    "cs_dataset = build_dataset(data_cfg.data.train) # must have 'type'\n",
    "cs_dataloader = build_dataloader(dataset=cs_dataset,\n",
    "                                samples_per_gpu=2,\n",
    "                                workers_per_gpu=1)\n",
    "for i, data in enumerate(cs_dataloader):\n",
    "    if i == 0:\n",
    "        print('data.img_metas:',data['img_metas'].data[0][0]['filename'])\n",
    "        print('gt',data['gt_semantic_seg'].data[0])\n",
    "        print('gt.size', data['gt_semantic_seg'].data[0].size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef72f64",
   "metadata": {},
   "source": [
    "## 稀有类抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb57f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 19:20:53,563 - mmseg - INFO - Loaded 2500 images from /raid/wzq/data/GTA5/images\n",
      "2022-10-21 19:20:53,664 - mmseg - INFO - Loaded 2975 images from /raid/wzq/data/cityscapes/leftImg8bit/train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source.gt: DataContainer(tensor([[[ 10,  10,  10,  ...,   2,   2,   2],\n",
      "         [ 10,  10,  10,  ...,   2,   2,   2],\n",
      "         [ 10,  10,  10,  ...,   2,   2,   2],\n",
      "         ...,\n",
      "         [255, 255, 255,  ..., 255, 255, 255],\n",
      "         [255, 255, 255,  ..., 255, 255, 255],\n",
      "         [255, 255, 255,  ..., 255, 255, 255]]]))\n"
     ]
    }
   ],
   "source": [
    "from mmseg.datasets.builder import build_dataset\n",
    "from mmcv.utils.config import Config\n",
    "\n",
    "cfg = Config.fromfile('/raid/wzq/code/DAFormer-master/configs/_base_/datasets/uda_gta_to_cityscapes_512x512.py')\n",
    "uda_dataset = build_dataset(cfg.data.train)\n",
    "print('source.gt:', uda_dataset[0]['gt_semantic_seg'])\n",
    "# 255的意思是无效标签，这对于我们的处理应该是有借鉴意义的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244b920",
   "metadata": {},
   "source": [
    "### 获取数据出现的频率并计算数据的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5196d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有类别的像素数： {17: 5445705, 12: 7444743, 6: 11509768, 16: 12863901, 15: 12995272, 14: 14774826, 18: 22848390, 7: 30521277, 3: 36211593, 4: 48485660, 9: 63964556, 11: 67201112, 5: 67767822, 10: 221461664, 1: 336037810, 13: 386482742, 8: 878719065, 2: 1259785820, 0: 2036071935}\n",
      "rcs类别index:\n",
      " [17, 12, 6, 16, 15, 14, 18, 7, 3, 4, 9, 11, 5, 10, 1, 13, 8, 2, 0]\n",
      "rcs抽样概率：\n",
      " [1.12773545e-01 1.08763158e-01 1.01042517e-01 9.85934958e-02\n",
      " 9.83590856e-02 9.52391103e-02 8.22818428e-02 7.16045201e-02\n",
      " 6.45916462e-02 5.17154671e-02 3.90705504e-02 3.68459150e-02\n",
      " 3.64694707e-02 2.25347024e-03 2.82815512e-04 1.13413160e-04\n",
      " 1.52171964e-08 1.52951003e-11 1.19580108e-17]\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import json\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "def get_rcs_class_probs(data_root, temperature):\n",
    "    with open(osp.join(data_root, 'sample_class_stats.json'), 'r') as of:\n",
    "        sample_class_stats = json.load(of)\n",
    "    overall_class_stats = {}\n",
    "    for s in sample_class_stats:\n",
    "        s.pop('file')\n",
    "        for c, n in s.items():\n",
    "            c = int(c)\n",
    "            if c not in overall_class_stats:\n",
    "                overall_class_stats[c] = n\n",
    "            else:\n",
    "                overall_class_stats[c] += n\n",
    "    #  到这一步计算出了所有类别的总像素数：{0：610000,1：3254..}\n",
    "    overall_class_stats = {\n",
    "        k: v\n",
    "        for k, v in sorted(\n",
    "            overall_class_stats.items(), key=lambda item: item[1])\n",
    "    }\n",
    "    print('所有类别的像素数：', overall_class_stats)\n",
    "    # 计算出频率，加温度后计算为概率\n",
    "    freq = torch.tensor(list(overall_class_stats.values()))\n",
    "    freq = freq / torch.sum(freq)\n",
    "    freq = 1 - freq\n",
    "    freq = torch.softmax(freq / temperature, dim=-1)\n",
    "    return list(overall_class_stats.keys()), freq.numpy()\n",
    "\n",
    "data_root = '/raid/wzq/data/cityscapes/'\n",
    "rcs_classes, rcs_classprob = get_rcs_class_probs(data_root=data_root,\n",
    "                         temperature=0.01) # 倾向于更极端的分布\n",
    "print('rcs类别index:\\n',rcs_classes)\n",
    "print('rcs抽样概率：\\n',rcs_classprob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69362c",
   "metadata": {},
   "source": [
    "### 完成概率->文件的转换\n",
    "\n",
    "实际上这里要做的是，抽样只能抽样概率，所有的概率分布都是最后的np.choice来确定的，但是抽样只能抽出c类别来，得从当前这个类当中获取出文件的集合，即：{0：\\[file1.png, file2.png.....\\]}，然后再以均匀分布抽出1个来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2acce3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标记各个类别在各个图像上的像素数：\n",
      " 0类别：\n",
      " [['/raid/wzq/data/GTA5/labels/00001_labelTrainIds.png', 667094], ['/raid/wzq/data/GTA5/labels/00002_labelTrainIds.png', 991166], ['/raid/wzq/data/GTA5/labels/00003_labelTrainIds.png', 784621]] \n",
      " 1类别：\n",
      " [['/raid/wzq/data/GTA5/labels/00002_labelTrainIds.png', 120392], ['/raid/wzq/data/GTA5/labels/00003_labelTrainIds.png', 103264], ['/raid/wzq/data/GTA5/labels/00004_labelTrainIds.png', 39781]]\n",
      "class  17 has  106 samples\n",
      "class  12 has  80 samples\n",
      "class  6 has  652 samples\n",
      "class  16 has  116 samples\n",
      "class  15 has  148 samples\n",
      "class  14 has  1040 samples\n",
      "class  18 has  20 samples\n",
      "class  7 has  404 samples\n",
      "class  3 has  1695 samples\n",
      "class  4 has  1134 samples\n",
      "class  9 has  1878 samples\n",
      "class  11 has  468 samples\n",
      "class  5 has  2338 samples\n",
      "class  10 has  2458 samples\n",
      "class  1 has  2077 samples\n",
      "class  13 has  1910 samples\n",
      "class  8 has  2420 samples\n",
      "class  2 has  2333 samples\n",
      "class  0 has  2495 samples\n",
      "02203_labelTrainIds.png : 0\n",
      "01989_labelTrainIds.png : 1\n",
      "01438_labelTrainIds.png : 2\n",
      "02375_labelTrainIds.png : 3\n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('/raid/wzq/code/DAFormer-master/configs/_base_/datasets/uda_gta_to_cityscapes_512x512.py')\n",
    "with open(\n",
    "        osp.join(cfg.data.train['source']['data_root'],\n",
    "                 'samples_with_class.json'), 'r') as of:\n",
    "    samples_with_class_and_n = json.load(of)\n",
    "print('标记各个类别在各个图像上的像素数：\\n','0类别：\\n',\n",
    "      samples_with_class_and_n['0'][:3],'\\n','1类别：\\n',\n",
    "      samples_with_class_and_n['1'][:3])\n",
    "\n",
    "# 转数字标签\n",
    "samples_with_class_and_n = {\n",
    "                int(k): v\n",
    "                for k, v in samples_with_class_and_n.items()\n",
    "                if int(k) in rcs_classes\n",
    "}\n",
    "\n",
    "# 计算带有class信息的samples信息\n",
    "samples_with_class = {}\n",
    "rcs_min_pixels = 3000 \n",
    "# 设置这个的目的是不希望采样到很少像素的类别图像\n",
    "# 例如，图像1只有2000个像素是train，那么这个图像就不宜被选为train的训练图像\n",
    "for c in rcs_classes:\n",
    "    samples_with_class[c] = []\n",
    "    for file, pixels in samples_with_class_and_n[c]:\n",
    "        if pixels > rcs_min_pixels:\n",
    "            samples_with_class[c].append(file)\n",
    "    assert len(samples_with_class[c]) > 0 \n",
    "    # 保证有取到的，不然的话要降低阈值\n",
    "    # 3000的时候，类18有20个图像，5000的时候，类18只有11个图像了\n",
    "for c in samples_with_class:\n",
    "    print('class ', c, 'has ', len(samples_with_class[c]), 'samples')\n",
    "    \n",
    "# 完成file2idx的转换\n",
    "# 这是因为，抽样得到的只能是idx，而不能是直接的filename, 这里是全局的\n",
    "file_to_idx = {}\n",
    "\n",
    "for i, dic in enumerate(uda_dataset.source.img_infos):\n",
    "    file = dic['ann']['seg_map']\n",
    "    if isinstance(uda_dataset.source, CityscapesDataset):\n",
    "        # CS->ACDC的原因，gta5的不考虑\n",
    "        file = file.split('/')[-1]\n",
    "    file_to_idx[file] = i\n",
    "for i,key in enumerate(file_to_idx):\n",
    "    print(key, ':', file_to_idx[key])\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfd3e5",
   "metadata": {},
   "source": [
    "### 抽样的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da23fd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choiced class is 6\n",
      "/raid/wzq/data/GTA5/labels/01283_labelTrainIds.png\n",
      "global index: 1412\n",
      "/raid/wzq/data/GTA5/images/01283.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 根据概率抽取类别：param1=[类别]，param2=[类别对应的概率]\n",
    "c = np.random.choice(rcs_classes, p=rcs_classprob)\n",
    "print(f'The choiced class is {c}')\n",
    "\n",
    "# 从当前类别当中抽取idx, 不指定概率，其实这个里面有个做法，让像素多的靠前\n",
    "f1 = np.random.choice(samples_with_class[c])\n",
    "print(f1) # 这个图像里包含>3000个像素的当前类\n",
    "f1 = f1.split('/')[-1] # 保持路径的一直\n",
    "\n",
    "# 从source中获取idx\n",
    "i1 = file_to_idx[f1]\n",
    "print('global index:', i1)\n",
    "s1 = uda_dataset.source[i1] # 直接取得s1的信息\n",
    "print(s1['img_metas'].data['filename'])\n",
    "\n",
    "# 从target中获取idx\n",
    "i2 = np.rasamples_with_classndom.choice(len(uda_dataset.target))\n",
    "s2 = uda_dataset.target[i2]\n",
    "\n",
    "result = {\n",
    "    **s1,\n",
    "    'target_img_metas': s2['img_metas'],\n",
    "    'target_img': s2['img'] # 区分\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b42e62",
   "metadata": {},
   "source": [
    "# 训练过程（ema+model+imnet）\n",
    "\n",
    "训练的时候，调用的是build_train_model构建一个dacs对象，这个对象的父类是一个dacs_decorate，父类自带了model，子类加上了其他训练范式的参数，并且加上了2个model：EMA_model(教师网络)和一个特征距离计算网络imnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd1aa6",
   "metadata": {},
   "source": [
    "## 构建train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db611f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/wzq/code/DAFormer-master/mmseg/models/backbones/mix_transformer.py:217: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model type is:\n",
      " <class 'mmseg.models.uda.dacs.DACS'>\n",
      "ema_model_class:\n",
      " <class 'mmseg.models.segmentors.encoder_decoder.EncoderDecoder'>\n",
      "imnet_model_class:\n",
      " <class 'mmseg.models.segmentors.encoder_decoder.EncoderDecoder'>\n"
     ]
    }
   ],
   "source": [
    "from mmseg.models.builder import build_train_model\n",
    "\n",
    "cfg_path_model = '/raid/wzq/code/DAFormer-master/configs/_base_/models/daformer_aspp_mitb5.py'\n",
    "cfg_path_uda_paradim = '/raid/wzq/code/DAFormer-master/configs/_base_/uda/dacs_a999_fdthings_for_learn.py'\n",
    "# 在build_train_model里面，是从整体的cfg里面读取内容，因此学习测试的时候需要往dacs配置文件里面加上model和runner的内容\n",
    "\n",
    "cfg = Config.fromfile(cfg_path_uda_paradim)\n",
    "\n",
    "train_model = build_train_model(cfg)\n",
    "print('train model type is:\\n', train_model.__class__) # 发现是DACS\n",
    "\n",
    "print('ema_model_class:\\n', train_model.get_ema_model().__class__)\n",
    "print('imnet_model_class:\\n', train_model.get_imnet_model().__class__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1beb36",
   "metadata": {},
   "source": [
    "### 在source上训练\n",
    "\n",
    "这只需要get_model，传播到model里面，无需传播给其他两个模型，这也算是一个梯度控制吧，因为过get_model模型的数据，反向传播的时候，也只会给model。此外注意，这里的操作全部是在forward_train里面进行的。这个操作我们不希望交给mmcv来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a76d5acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_loss:2.7588908672332764\n",
      "clean_log_vars:OrderedDict([('features', 1.645821701146133e-09), ('decode.loss_seg', 2.7588908672332764), ('decode.acc_seg', 2.486419677734375), ('loss', 2.7588908672332764)])\n"
     ]
    }
   ],
   "source": [
    "def train_on_source(train_model, img, img_metas, gt_semantic_seg,\n",
    "                 target_img_metas, target_img):\n",
    "    clean_losses = train_model.get_model().forward_train(\n",
    "        img, img_metas, gt_semantic_seg, return_feat=True\n",
    "    )\n",
    "    clean_loss, clean_log_vars = train_model._parse_losses(clean_losses)\n",
    "#     clean_loss.backward(retain_graph=True)\n",
    "    print(f'clean_loss:{clean_loss}\\nclean_log_vars:{clean_log_vars}')\n",
    "\n",
    "from mmseg.datasets.builder import build_dataloader\n",
    "\n",
    "uda_dataloader = build_dataloader(dataset=uda_dataset,\n",
    "                                 samples_per_gpu=2,\n",
    "                                 workers_per_gpu=2)\n",
    "for i,result in enumerate(uda_dataloader):\n",
    "#     print(result)\n",
    "    train_on_source(train_model,\n",
    "                   img=result['img'].data[0],\n",
    "                   img_metas=result['img_metas'],\n",
    "                   gt_semantic_seg=result['gt_semantic_seg'].data[0],\n",
    "                   target_img=result['target_img'].data[0],\n",
    "                   target_img_metas=result['target_img_metas'])\n",
    "    if i == 0: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84b426",
   "metadata": {},
   "source": [
    "### 计算ImageNet上的Feature distance loss\n",
    "\n",
    "这一部分的核心在于使用固定好的imagnet预训练的imnet来前向一个feature，然后用这个feature来对目前的特征正则化。但是，imagenet和cityscapes在一些类别上是不对应的，因此作者采用了一个mask的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d88b4acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_rescaled.size()=torch.Size([2, 1, 16, 16])\n",
      "feat_loss:0.15826842188835144\n",
      "feat_log:OrderedDict([('loss_imnet_feat_dist', 0.15826842188835144), ('loss', 0.15826842188835144)])\n"
     ]
    }
   ],
   "source": [
    "imnet_feature_dist_lambda=0.005\n",
    "imnet_feature_dist_classes=[6, 7, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "imnet_feature_dist_scale_min_ratio=0.75\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def downscale_label_ratio(gt,\n",
    "                          scale_factor,\n",
    "                          min_ratio,\n",
    "                          n_classes,\n",
    "                          ignore_index=255):\n",
    "    assert scale_factor > 1\n",
    "    bs, orig_c, orig_h, orig_w = gt.shape\n",
    "    assert orig_c == 1\n",
    "    trg_h, trg_w = orig_h // scale_factor, orig_w // scale_factor\n",
    "    ignore_substitute = n_classes\n",
    "\n",
    "    out = gt.clone()  # otw. next line would modify original gt\n",
    "    out[out == ignore_index] = ignore_substitute\n",
    "    out = F.one_hot(\n",
    "        out.squeeze(1), num_classes=n_classes + 1).permute(0, 3, 1, 2)\n",
    "    assert list(out.shape) == [bs, n_classes + 1, orig_h, orig_w], out.shape\n",
    "    out = F.avg_pool2d(out.float(), kernel_size=scale_factor)\n",
    "    gt_ratio, out = torch.max(out, dim=1, keepdim=True)\n",
    "    out[out == ignore_substitute] = ignore_index\n",
    "    out[gt_ratio < min_ratio] = ignore_index # add_prefix如果gt_ratio比最小比值还小的话，就认为是ignore的\n",
    "    assert list(out.shape) == [bs, 1, trg_h, trg_w], out.shape\n",
    "    return out\n",
    "\n",
    "def masked_feat_dist(f1, f2, mask=None):\n",
    "    # 这个里面非常的简单，就是L2\n",
    "    feat_diff = f1 - f2\n",
    "    pw_feat_dist = torch.norm(feat_diff, dim=1, p=2)\n",
    "    if mask is not None:\n",
    "        pw_feat_dist = pw_feat_dist[mask.squeeze(1)] # mask[2,1,16,16]->[2,16,16]\n",
    "    return torch.mean(pw_feat_dist) # 最终算一个mean得到\n",
    "\n",
    "def cal_feature_distance(img, gt, src_feat):\n",
    "    \"\"\"\n",
    "        这里的src_feat是前面model生成的，img要被投入到imnet中生成特征，\n",
    "        这样计算出来的loss才能保证接着被只传入到model中反向传播\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        train_model.get_imnet_model().eval() # 固定imnet的权重\n",
    "        feat_imnet = train_model.get_imnet_model().extract_feat(img)# 提取预训练下的class特征\n",
    "        feat_imnet = [f.detach() for f in feat_imnet] # 4层的特征\n",
    "    lay = -1\n",
    "    fdist_classes = imnet_feature_dist_classes # 配置文件给到的\n",
    "    fdclasses = torch.tensor(fdist_classes,device=gt.device)\n",
    "    scale_factor = gt.shape[-1]//src_feat[lay].shape[-1] # 32\n",
    "    # 这里是用最后一个尺度的特征做正则化\n",
    "    # gt的尺寸是最后一个尺度的32倍\n",
    "    gt_rescaled = downscale_label_ratio(gt,\n",
    "                                       scale_factor,\n",
    "                                       imnet_feature_dist_scale_min_ratio,\n",
    "                                       n_classes=19,\n",
    "                                       ignore_index=255)\n",
    "    print(f'gt_rescaled.size()={gt_rescaled.shape}')\n",
    "    \n",
    "    # 处理和imagenet不一样的类别了\n",
    "    # 对于语义分割任务，类别是分布在图像的各个区域的，只要把需要忽视的区域设置为-1就可以了\n",
    "    fdist_mask = torch.any(gt_rescaled[..., None]==fdclasses, -1)\n",
    "    \"\"\"\n",
    "    这个非常trick\n",
    "    \n",
    "    知识点1：torch.any\n",
    "        torch.any(input, dim)的用法为：\n",
    "        对于input[dim]上的所有元素判断为true还是false\n",
    "        只要input[dim]上有一个元素判断为true，则该dim上认定为true，并合并为1个\n",
    "        例如，[3,3,2]，torch.any(input,dim=-1)，则会合并为一个[3,3]的张量\n",
    "    \n",
    "    知识点2：tensor[...,None]->标量化\n",
    "        此处，gt_rescaled的维度为[2, 1, 16, 16]\n",
    "        给一个None的意思是，拓展一个维度，变成[2, 1, 16, 16, 1]\n",
    "        这个时候，gt_rescaled[..., None]的意思就是将维度变成1，也就是进行逐个元素的考虑，这是比较trick的一个做法\n",
    "    \n",
    "    知识点3：tensor广播\n",
    "        gt_rescaled[..., None] == fdclasses\n",
    "        左边是标量值，将遍历这个tensor，右边是一个tensor列表\n",
    "        注意，右边一定要是列表，否则无法执行广播操作，同样，左边也要有一个1维度\n",
    "        例如，当前左边取出的值是16，16==fdclasses得到的结果，将是1个bool的len(fdclasses)的列表（被广播）\n",
    "        也就是说，如果当前类（如16）在fdclasses里面的话，\n",
    "        这个列表将出现一个True的值，整个any的值也为True\n",
    "        当前类（如1），不在fdclasses里面，全部都是False，any过后也是False\n",
    "    \n",
    "    整体流程：\n",
    "        所以，当[2,1,16,16]经过None之后，[2,1,16,16,1]\n",
    "        得到了1个广播施加上去后，变成了[2,1,16,16,10=len(fdclasses)]\n",
    "        再在最后一个维度上any，就得到了“当前这个像素是不是在fdclasses里面”了\n",
    "        并且维度重新回到了[2,1,16,16]的bool，标记着图像的哪些部分是在fdclasses类别中\n",
    "        实际上，[...,None]的意思，就是要“做广播”了。\n",
    "    \"\"\"\n",
    "    # 在mask的帮助下计算feature distance\n",
    "    # 进入计算的都是最后一层的高层特征，也就是list[4][-1]\n",
    "    # [2,512,16,16] 512层的通道中包含了丰富的类别信息\n",
    "    # 而且通道确实有“类别”的意思在里面\n",
    "    feat_dist = masked_feat_dist(src_feat[-1], feat_imnet[-1], mask=fdist_mask)\n",
    "    feat_dist = imnet_feature_dist_lambda * feat_dist # 大费周章结果才0.05的权重\n",
    "    feat_loss, feat_log = train_model._parse_losses(\n",
    "                            {'loss_imnet_feat_dist': feat_dist})\n",
    "    return feat_loss, feat_log\n",
    "    \n",
    "\n",
    "def train_by_feature_distance(train_model, img, img_metas, gt_semantic_seg,\n",
    "                 target_img_metas, target_img):\n",
    "    # 先生成在src上的特征\n",
    "    src_feat = train_model.get_model().extract_feat(img)\n",
    "    feat_loss, feat_log = cal_feature_distance(\n",
    "        img, gt_semantic_seg, src_feat)\n",
    "#     feat_loss.backward()\n",
    "    print(f'feat_loss:{feat_loss}\\nfeat_log:{feat_log}')\n",
    "\n",
    "\n",
    "# 标准测试框架\n",
    "for i,result in enumerate(uda_dataloader):\n",
    "#     print(result)\n",
    "    train_by_feature_distance(train_model,\n",
    "                   img=result['img'].data[0],\n",
    "                   img_metas=result['img_metas'],\n",
    "                   gt_semantic_seg=result['gt_semantic_seg'].data[0],\n",
    "                   target_img=result['target_img'].data[0],\n",
    "                   target_img_metas=result['target_img_metas'])\n",
    "    if i == 0: \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5c3d5",
   "metadata": {},
   "source": [
    "### 生成pseudo label\n",
    "\n",
    "伪标签是EMA-model也就是教师网络生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8056c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "pseudo_weight:\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "gt_pixel_weight:\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 超参数：多少伪标签的概率才可信,很苛刻啊\n",
    "pesudo_threshold=0.968 \n",
    "\n",
    "def generate_pesudo_label(train_model, img, img_metas, gt_semantic_seg,\n",
    "                 target_img_metas, target_img):\n",
    "    # 冻结ema_model的权重：\n",
    "    for m in train_model.get_ema_model().modules():\n",
    "        if isinstance(m, _DropoutNd):\n",
    "            m.training = False\n",
    "        if isinstance(m, DropPath):\n",
    "            m.training = False\n",
    "    \n",
    "    # 生成最初的分割结果[2, 19, 512, 512], 是有负数的\n",
    "    ema_logits = train_model.get_ema_model().encode_decode(target_img, target_img_metas)\n",
    "    # softmax\n",
    "    ema_softmax = torch.softmax(ema_logits.detach(), dim=1)\n",
    "    # 得到置信度和概率,这里的概率是要用到的，因为只有概率比较高的伪标签才可信\n",
    "    pseudo_prob, pseudo_label = torch.max(ema_softmax, dim=1)\n",
    "    ps_large_p = pseudo_prob.ge(pseudo_prob).long()==1 # 好苛刻啊，这个调的第一点可不可以\n",
    "    # long的意思是把True和False转成0还是1，长整型\n",
    "    \n",
    "    # 而后计算伪标签的权重\n",
    "    pseudo_weight = torch.sum(ps_large_p).item()/np.size(np.array(pseudo_label.cpu()))\n",
    "    print(pseudo_weight)\n",
    "    pseudo_weight = pseudo_weight * torch.ones(pseudo_prob.shape)\n",
    "  \n",
    "    # 不信任在边界上生成的伪标签\n",
    "    if train_model.psweight_ignore_top > 0:\n",
    "        # Don't trust pseudo-labels in regions with potential\n",
    "        # rectification artifacts. This can lead to a pseudo-label\n",
    "        # drift from sky towards building or traffic light.\n",
    "        pseudo_weight[:, :train_model.psweight_ignore_top, :] = 0\n",
    "    if train_model.psweight_ignore_bottom > 0:\n",
    "        pseudo_weight[:, -train_model.psweight_ignore_bottom:, :] = 0\n",
    "    gt_pixel_weight = torch.ones((pseudo_weight.shape))\n",
    "    print(f'pseudo_weight:\\n{pseudo_weight}')\n",
    "    print(f'gt_pixel_weight:\\n{gt_pixel_weight}')\n",
    "    \n",
    "from torch.nn.modules.dropout import _DropoutNd\n",
    "from timm.models.layers.drop import DropPath\n",
    "    \n",
    "# 标准测试框架\n",
    "for i,result in enumerate(uda_dataloader):\n",
    "#     print(result)\n",
    "    generate_pesudo_label(train_model,\n",
    "                   img=result['img'].data[0],\n",
    "                   img_metas=result['img_metas'],\n",
    "                   gt_semantic_seg=result['gt_semantic_seg'].data[0],\n",
    "                   target_img=result['target_img'].data[0],\n",
    "                   target_img_metas=result['target_img_metas'])\n",
    "    if i == 0: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2a099",
   "metadata": {},
   "source": [
    "### 将标签和图像混合并训练\n",
    "\n",
    "这个部分将图像，伪标签、真值标签全部混合起来，从而强迫网络学习到两个不一样的特征，使用的是strong_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fbd5dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: tensor([[[15, 15, 15,  ..., 15, 15, 15],\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]])\n",
      "after unique opt: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13,  14,\n",
      "         15, 255])\n",
      "label: tensor([[[  8,   8,   8,  ...,   2,   2,   2],\n",
      "         [  8,   8,   8,  ...,   2,   2,   2],\n",
      "         [  8,   8,   8,  ...,   2,   2,   2],\n",
      "         ...,\n",
      "         [255, 255, 255,  ..., 255, 255, 255],\n",
      "         [255, 255, 255,  ..., 255, 255, 255],\n",
      "         [255, 255, 255,  ..., 255, 255, 255]]])\n",
      "after unique opt: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13,  14,\n",
      "         15, 255])\n",
      "mixed_img: torch.Size([1, 3, 512, 512])\n",
      "mixed_lbl: torch.Size([1, 1, 512, 512])\n",
      "mixed_img: torch.Size([1, 3, 512, 512])\n",
      "mixed_lbl: torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "strong_parameters = {\n",
    "    'mix': None,\n",
    "    'color_jitter': np.random.uniform(0, 1),\n",
    "    'color_jitter_s': train_model.color_jitter_s,\n",
    "    'color_jitter_p': train_model.color_jitter_p,\n",
    "    'blur': np.random.uniform(0, 1) if train_model.blur else 0,\n",
    "#     'mean': means[0].unsqueeze(0),  # assume same normalization\n",
    "#     'std': stds[0].unsqueeze(0)\n",
    "}\n",
    "\n",
    "def generate_classes_mask(label, classes):\n",
    "    # 进来的时候，label是[1,512,512]\n",
    "    # classes 是[....], 即筛选出的一般类别的index\n",
    "    \n",
    "    label, classes = torch.broadcast_tensors(label,\n",
    "                                            classes.unsqueeze(1).unsqueeze(2))\n",
    "    \"\"\"\n",
    "        torch.broadcast_tensors(a, b)的用法为：\n",
    "        做了若干次广播操作，把两个张量都广播为最大的尺度\n",
    "        例如a为[1,512,512], b[8,1,1]，在3个维度上全部广播\n",
    "        得到a为[8,512,512], b[8,512,512]\n",
    "        \n",
    "        # label是8个512*512的类别\n",
    "        # classes是8个512*512的单值矩阵\n",
    "        相当于问题升了一个维度，eq一下就可以获得各层的掩码了\n",
    "        而后再将8层进行相加，就得到了标记的class_mask \n",
    "    \"\"\"\n",
    "    class_mask = label.eq(classes).sum(dim=0, keepdims=True)\n",
    "    return class_mask # list:[[1,1,512,512],[1,1,512,512]]\n",
    "    \n",
    "def get_class_masks(labels):\n",
    "    class_masks = []\n",
    "    for label in labels:\n",
    "        print('label:', label) # [1, 512,512]\n",
    "        classes = torch.unique(labels)\n",
    "        # torch.unique的意思是挑出“独立不重复”的元素\n",
    "        print('after unique opt:', classes) # [0,1,...17]\n",
    "        nclasses = classes.shape[0]\n",
    "        \n",
    "        # 选择一部分元素，这里的意思应该是避免选择天空之类的？\n",
    "        class_choice = np.random.choice(\n",
    "            nclasses, int((nclasses + nclasses % 2) / 2), replace=False)\n",
    "        \"\"\"\n",
    "            np.random.choice的用法有：\n",
    "                1、产生随机数（参数1=int，参数2=int）\n",
    "                    这里就是从（0，15）中随机产生8个数，replace=False表示不取相同数字\n",
    "                2、根据概率从列表中抽取（参数1=list，参数2=prob_list）\n",
    "                    就是在rcs里面的用法\n",
    "            在每次迭代过程中，class_mix采用的类别是不一样的，\n",
    "            因此就采用这种随机抽取的方法来实现鲁棒性\n",
    "        \"\"\"\n",
    "        # 抽得8个类别（其实是一半的类别）后，进入generate_class_mask阶段\n",
    "        # 这一阶段的主要目的是为了得到“对应于当前8个类的bool mask”\n",
    "        \n",
    "        classes = classes[torch.Tensor(class_choice).long()] # 15->8\n",
    "        class_masks.append(generate_classes_mask(label, classes).unsqueeze(dim=0))\n",
    "    return class_masks # 对应batch中各个图片的cut类别bool\n",
    "\n",
    "# 下面是混合的步骤\n",
    "def one_mix(mask, data=None, target=None):\n",
    "    # 这里要注意的是，对于data而言，进来的都是彩色图像3通道的\n",
    "    # 对于target而言，进来的都是单通道的，\n",
    "    # 需要把label_mask先拓展到3通道上，省事也用了broadcast\n",
    "    # 标签是来自于源域的，因此1的部分只能乘在source上\n",
    "    # 然后1-mask表示0的部分乘在target上，从而实现了图像混合\n",
    "    \n",
    "    if not (data is None):\n",
    "        stackedMask0, _ = torch.broadcast_tensors(mask[0], \n",
    "                                                 data[0])\n",
    "        data = (stackedMask0 * data[0] + (1 - stackedMask0)*data[1]).unsqueeze(0)\n",
    "    if not (target is None):\n",
    "        stackedMask0, _ = torch.broadcast_tensors(mask[0],\n",
    "                                                 target[0])\n",
    "        target = (stackedMask0 * target[0] + (1 - stackedMask0)*target[1]).unsqueeze(0)\n",
    "    return data, target\n",
    "        \n",
    "def strong_transform(mix_params, data, target):\n",
    "    data, target = one_mix(mask=mix_params['mix'],data=data, target=target)\n",
    "    return data, target\n",
    "    \n",
    "def mixing_img_label_and_train(train_model, img, img_metas, gt_semantic_seg,\n",
    "                 target_img_metas, target_img):\n",
    "    mixed_img, mixed_lbl = [None] * 2, [None] * 2 # [None,None]\n",
    "    \n",
    "    # 根据类别获取到cut的信息，这里是class-Mix论文中写到的\n",
    "    mix_masks = get_class_masks(gt_semantic_seg) # [2,1,512,512]\n",
    "    for i in range(batch_size):\n",
    "        strong_parameters['mix'] = mix_masks[i]\n",
    "        mixed_img[i], mixed_lbl[i] = strong_transform(\n",
    "            strong_parameters,\n",
    "            data=torch.stack((img[i], target_img[i])),\n",
    "            target=torch.stack((gt_semantic_seg[i][0],torch.randn(2,512,512)[i])))\n",
    "            # 伪标签占位符， 注意gt的格式是[2,1,512,512]\n",
    "        print(\"mixed_img:\", mixed_img[i].shape)\n",
    "        print('mixed_lbl:', mixed_lbl[i].shape)\n",
    "\n",
    "# 最主要的是这个混合，高斯模糊和色彩扰动后续再调\n",
    "\n",
    "# 标准测试框架\n",
    "for i,result in enumerate(uda_dataloader):\n",
    "#     print(result)\n",
    "    mixing_img_label_and_train(train_model,\n",
    "                   img=result['img'].data[0],\n",
    "                   img_metas=result['img_metas'],\n",
    "                   gt_semantic_seg=result['gt_semantic_seg'].data[0],\n",
    "                   target_img=result['target_img'].data[0],\n",
    "                   target_img_metas=result['target_img_metas'])\n",
    "    if i == 0: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a529ae",
   "metadata": {},
   "source": [
    "# 补充知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d61a5b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([ True, False])\n",
      "tensor([[[ True],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]]])\n",
      "tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([\n",
    "    [True, False],\n",
    "    [False, False]\n",
    "])\n",
    "print(x.shape)\n",
    "print(torch.any(x, dim=1))\n",
    "print(x[...,None]) # 拉成1个值\n",
    "\n",
    "torch.broadcast_tensors\n",
    "print(torch.unique(torch.tensor([2,3,2,4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d566f806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
